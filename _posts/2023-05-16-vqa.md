---
layout: post
title: VQA in a Toy World
subtitle: 6.8300 Final Project
thumbnail-img: /assets/img/vqa.png
mathjax: true
---

This was my team's final project for 6.8300, Advances in Computer Vision, where we created a multi-model pipeline to answer multiple-choice questions regarding an input image / scene.

## Introduction

Visual question answering is the process of answer a question given an input image and question.

The current standard utilizes a CNN + Transformer / LSTM:

![overfit](/assets/img/cnn-and-lstm.png){: .mx-auto.d-block :}

We wanted to try something fun: can a combination of two different models utilizing image captioning (CV) and question answering (NLP) match or get near the SoTA?

## Methodology

1. Generate detailed caption from input image

2. Answer multiple choice question given context from caption

![overfit](/assets/img/cv-pipeline.png){: .mx-auto.d-block :}

## Experimentation

Here is a sample output from the first half, where we generate a caption from the image and compare it to the ground truth caption for that image.

![overfit](/assets/img/image-captioning.png){: .mx-auto.d-block :}

You can see that while it's not too bad, there is loss of information when compared to the ground truth caption. The question answering model thus misses out on critical information needed to answer the question.

## Reflection

As expected, our results were not too good (although our pipeline performed better than random guessing). The image-to-caption transition loses a lot of information unless the model output is extremely detailed and precise, and so it serves as a bottleneck in the pipeline's performance. It was a fun attempt though.
