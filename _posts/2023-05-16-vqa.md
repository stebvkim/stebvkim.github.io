---
layout: post
title: VQA in a Toy World
subtitle: 6.8300 Final Project
thumbnail-img: /assets/img/vqa.png
mathjax: true
---

This was my team's final project for 6.8300, Advances in Computer Vision, where we created a multi-model pipeline to answer multiple-choice questions regarding an input image / scene from a toy dataset.

## Introduction

Visual question answering is a computer vision field where a model provides an answer given an input image and text-based question.

Current approaches typically utilize a CNN + Transformer / LSTM:

![overfit](/assets/img/cnn-and-lstm.png){: .mx-auto.d-block :}

We wanted to try something fun: could a combination of two different models utilizing image captioning (CV) and question answering (NLP) perform at the same level as or near the SoTA?

## Methodology

1. Generate a very detailed caption from the input image

2. Answer the multiple choice question given the context from the caption

![overfit](/assets/img/cv-pipeline.png){: .mx-auto.d-block :}

## Experimentation

Here is a sample output from the first half of the pipeline, where we generate a caption from the image and compare it to the ground truth caption for that image.

![overfit](/assets/img/image-captioning.png){: .mx-auto.d-block :}

You can see that while it's not too bad, there is noticeable loss of information when compared to the ground truth caption (e.g. the generated caption fails to include the worried expression of the man). The question answering model thus misses out on critical information needed to answer certain questions.

## Reflection

As expected, our results were not too good (although our pipeline performed better than random guessing). The image-to-caption transition loses a lot of information unless the model output is extremely detailed and precise, and so it serves as a bottleneck in the pipeline's performance. It was a fun attempt though.
