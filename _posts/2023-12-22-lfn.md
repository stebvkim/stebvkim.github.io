---
layout: post
title: Light Field Networks
subtitle: 6.S980 Final Project
thumbnail-img: /assets/img/chair.png
mathjax: true
---

This was my team's final project for 6.S980, Machine Learning for Inverse Graphics, where we created a light field network (LFN).

## Introduction

We created a "toy" version of the light field network proposed in [Light Field Networks: Neural Scene Representations with
Single-Evaluation Rendering](https://www.vincentsitzmann.com/lfns/) by Sitzmann et al. (my professor for this class!). Additionally, we added multi-view consistency to allow for novel scene generation with learned latent codes. LFNs come with great speed advantages compared to neural radiance fields (NeRFs) because only a single call to a neural field is required to predict the color of a pixel, as opposed to having to do multiple calls for ray marching in NeRFs.

## Methodology

We first overfit the model on a single scene, where we were able to reconstruct training views of a single chair. This part was fairly simple and intuitive.

At this stage, the model didn't have novel view generation, as light fields do not have built-in multi-view consistency. To achieve this, we implemented an autodecoder to learn the latent space and latent codes for different scenes.

## Experimentation

Overfitting on a single scene:

![overfit](/assets/img/chair_overfit.png){: .mx-auto.d-block :}

## Final Output

Novel view generation from never-seen-before perspectives:

![overfit](/assets/img/novel_view_chair.png){: .mx-auto.d-block :}

## Final Thoughts

It could look better with more thorough training and implementation, but we were able to implement a fundamentally correct LFN, which was the goal of this project.
